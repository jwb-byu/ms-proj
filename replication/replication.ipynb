{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "848d4645",
   "metadata": {},
   "source": [
    "# Demonstration of Spike Train Analysis Algorithm Replication"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb9b8e2",
   "metadata": {},
   "source": [
    "Wesley Borden"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4580eb1c",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e1da28b",
   "metadata": {},
   "source": [
    "Here, I demonstrate the use of a set of python functions I have developed to replicate two algorithms. I also provide exploratory graph analysis and visualizations of the network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc884f37",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d668e5",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba7550d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import uuid\n",
    "from typing import Callable, Optional\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from brainbox.io.one import SpikeSortingLoader\n",
    "from iblutil.util import Bunch\n",
    "from one.alf.io import AlfBunch\n",
    "from one.api import OneAlyx, ONE  # Docs: https://int-brain-lab.github.io/ONE/\n",
    "\n",
    "from cc.cc import cross_correlate\n",
    "from gu.utils import adjacency_matrix_from_pairwise, show_graph\n",
    "from te.te import transfer_entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa2ee8e",
   "metadata": {},
   "source": [
    "### API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e445d80",
   "metadata": {},
   "source": [
    "IBL API as demonstrated in `.../data-demo/nsp_data_demo_jwb.ipynb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50633e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_alyx: OneAlyx = ONE(\n",
    "    cache_dir=\"/Users/wesley/GitHub/BYU/ms-proj/tmp/one-cache\",  # any directory where temporary files can be synced\n",
    "    base_url=\"https://openalyx.internationalbrainlab.org\",  # base url for the API\n",
    "    password=\"international\",  # public-access password\n",
    "    silent=True,  # don't print progress, etc.\n",
    ")  # most 'type: ignore' are because IBL's libraries are less strict on types # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac37d030",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tag = \"2024_Q2_IBL_et_al_BWM_iblsort\"  # tag for most recent data release ()\n",
    "all_sessions: list = one_alyx.search(  # list of sessions\n",
    "    tag=data_tag, query_type=\"remote\"\n",
    ")  # type: ignore\n",
    "n_sessions = len(all_sessions)\n",
    "print(f\"Session count: {n_sessions}\")\n",
    "print(f\"Session example: {all_sessions[0]}\")\n",
    "\n",
    "all_insertions: list = one_alyx.search_insertions(  # list of insertions\n",
    "    tag=data_tag, query_type=\"remote\"\n",
    ")  # type: ignore\n",
    "n_insertions = len(all_insertions)\n",
    "print(f\"Insertion count: {n_insertions}\")\n",
    "print(f\"Insertion example: {all_insertions[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab61f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the same one for consistency between this demo and other IBL demos\n",
    "pid_i = 534\n",
    "pid: str = str(all_insertions[pid_i])\n",
    "pid_details: tuple[str, str] = one_alyx.pid2eid(pid)\n",
    "eid, p_name = pid_details\n",
    "\n",
    "print(f\"Probe ID: {pid}\")\n",
    "print(f\"Probe Name: {p_name}\")\n",
    "print(f\"Experiment ID: {eid}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ebf0bb",
   "metadata": {},
   "source": [
    "### Load Spike-Sorted Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a020072f",
   "metadata": {},
   "source": [
    "As demonstrated in `.../data-demo/nsp_data_demo_jwb.ipynb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f430e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "spike_loader = SpikeSortingLoader(pid=pid, one=one_alyx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b66175",
   "metadata": {},
   "outputs": [],
   "source": [
    "spike_sorting_data: tuple[AlfBunch, AlfBunch, Bunch] = spike_loader.load_spike_sorting()  # type: ignore\n",
    "spikes, clusters, channels = spike_sorting_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4bfef89",
   "metadata": {},
   "outputs": [],
   "source": [
    "spikes_df = spikes.to_df()\n",
    "spikes_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ccc159c",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters_wrangled: dict = {}\n",
    "for k, v in clusters.items():\n",
    "    if v.ndim == 1:\n",
    "        clusters_wrangled[k] = v\n",
    "    elif v.ndim == 2:\n",
    "        for k_sub in v:\n",
    "            v_sub = v[k_sub]\n",
    "            clusters_wrangled[k_sub] = v_sub\n",
    "    else:\n",
    "        raise ValueError(\"Bad dimensions\")\n",
    "\n",
    "\n",
    "clusters_df: pd.DataFrame = pd.DataFrame(clusters_wrangled)\n",
    "clusters_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "758f5763",
   "metadata": {},
   "outputs": [],
   "source": [
    "channels_df = AlfBunch(channels).to_df()\n",
    "channels_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e15a777",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_clusters: AlfBunch = spike_loader.merge_clusters(spikes, clusters, channels)  # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "902ea0d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_clusters_df = merged_clusters.to_df()\n",
    "merged_clusters_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c99613",
   "metadata": {},
   "source": [
    "### Timeframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef2185e",
   "metadata": {},
   "source": [
    "As demonstrated in `.../data-demo/nsp_data_demo_jwb.ipynb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac7dc9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = 150  # seconds since beginning the electrophysiology recording\n",
    "end_time = 152  # seconds since beginning the electrophysiology recording"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a3235b",
   "metadata": {},
   "outputs": [],
   "source": [
    "spikes_df_timeframe = spikes_df[start_time <= spikes_df[\"times\"]]\n",
    "spikes_df_timeframe = spikes_df_timeframe[spikes_df_timeframe[\"times\"] <= end_time]\n",
    "spikes_df_timeframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dba34d1",
   "metadata": {},
   "source": [
    "### Determine Bin Size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efaf9da0",
   "metadata": {},
   "source": [
    "In `.../data-demo/nsp_data_demo_jwb.ipynb`, we used high-resolution bins that slowed down processing. For spike train analysis, we can tune bin size as a hyperparameter. We will use a 10ms bins size, which aligns with a prior study (Moore, 1970, Statistical Signs of Synaptic Interaction in Neurons, https://doi.org/10.1016/S0006-3495(70)86341-X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc061cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "bins_per_s = 100 # each bin is 10ms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fce258c",
   "metadata": {},
   "source": [
    "### Wrangle to Clusters-by-Time Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c55b433e",
   "metadata": {},
   "source": [
    "Adapted from `.../data-demo/nsp_data_demo_jwb.ipynb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d25c212d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_channel_map = (\n",
    "    merged_clusters_df[[\"cluster_id\", \"channels\"]]\n",
    "    .copy()\n",
    "    .sort_values(by=\"channels\", ascending=True)\n",
    "    .reset_index(drop=True)\n",
    "    .reset_index(drop=False)\n",
    "    .rename(inplace=False, columns={\"index\": \"cluster_channel_id\"})\n",
    ")\n",
    "cluster_channel_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a922fdff",
   "metadata": {},
   "outputs": [],
   "source": [
    "spikes_df_timeframe = spikes_df_timeframe.merge(\n",
    "    cluster_channel_map, left_on=\"clusters\", right_on=\"cluster_id\", how=\"left\"\n",
    ")\n",
    "spikes_df_timeframe[\"time_bin\"] = (np.floor((spikes_df_timeframe[\"times\"] - start_time) * bins_per_s)).astype(int)  # bin by microsecond\n",
    "spikes_df_timeframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e3ad9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters_spikes_matrix = np.zeros(\n",
    "    (cluster_channel_map.shape[0], ((end_time - start_time) * bins_per_s))\n",
    ")  # type: ignore\n",
    "clusters_spikes_matrix[\n",
    "    (\n",
    "        spikes_df_timeframe[\"cluster_channel_id\"].max()\n",
    "        - spikes_df_timeframe[\"cluster_channel_id\"].values\n",
    "    ),\n",
    "    spikes_df_timeframe[\"time_bin\"].values,\n",
    "] = int(1)  # 1 represents a spike # type: ignore\n",
    "clusters_spikes_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6caa56b",
   "metadata": {},
   "source": [
    "### Visualize Spike Trains"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09fe1b11",
   "metadata": {},
   "source": [
    "As demonstrated in `.../data-demo/nsp_data_demo_jwb.ipynb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3901ce1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "axs.scatter(\n",
    "    spikes_df_timeframe[\"times\"].values,  # type: ignore\n",
    "    spikes_df_timeframe[\"cluster_channel_id\"].values,  # type: ignore\n",
    "    s=1,\n",
    "    alpha=0.5,\n",
    "    c=\"#000000\",\n",
    "    marker=\"s\",\n",
    ")\n",
    "\n",
    "axs.set_title(\"Putative Neural Spikes\")\n",
    "axs.set_xlabel(\"Time (s)\")\n",
    "axs.set_ylabel(\"Putative Neuron\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f2fe173",
   "metadata": {},
   "source": [
    "### Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "532bc7be",
   "metadata": {},
   "source": [
    "Everything to this point has been copied or adapted from `.../data-demo/nsp_data_demo_jwb.ipynb`. Now we will show how to use the data to identify a biological neural network: a partial connectome."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d10305",
   "metadata": {},
   "source": [
    "## Cross Correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e685403",
   "metadata": {},
   "source": [
    "Cross correlation involves a sliding dot product of two vectors that represent parallel spike trains. The resulting distribution includes outliers if there is a significant correlation between the two spike trains. This is implemented in `cross_correlate`, which returns a category as follows:\n",
    "\n",
    "|Category | Meaning |\n",
    "|---|---|\n",
    "|  1| relationship |\n",
    "|  0| no relationship |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6915ca90",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Comparing a spike train to itself returns {cross_correlate(clusters_spikes_matrix[0], clusters_spikes_matrix[0])}\")\n",
    "print(f\"Comparing a spike train to a distant spike train returns {cross_correlate(clusters_spikes_matrix[0], clusters_spikes_matrix[-1])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a45e32f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_limit = 10\n",
    "sample_count = 0\n",
    "for i, _ in enumerate(clusters_spikes_matrix):\n",
    "    for j in range(i+1, min(i+100, len(clusters_spikes_matrix))):\n",
    "        if cross_correlate(clusters_spikes_matrix[i], clusters_spikes_matrix[j]):\n",
    "            sample_count += 1\n",
    "            print(f\"Spike train {i} is functionally connected to spike train {j}\")\n",
    "            if sample_count >= sample_limit:\n",
    "                break\n",
    "    if sample_count >= sample_limit:\n",
    "        break \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01991965",
   "metadata": {},
   "source": [
    "We can use cross correlation to construct an adjacency matrix and a graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56bd36f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "adjacency_matrix_from_cc = adjacency_matrix_from_pairwise(clusters_spikes_matrix, cross_correlate)\n",
    "g_from_cc = nx.from_numpy_array(adjacency_matrix_from_cc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68468834",
   "metadata": {},
   "source": [
    "Let's visualize the resulting graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab1cb9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_graph(g_from_cc, (5, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8cb05f9",
   "metadata": {},
   "source": [
    "That gave a lot of unconnected nodes. Let's look at the largest component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f1f1912",
   "metadata": {},
   "outputs": [],
   "source": [
    "largest_connected_component_nodes_cc = max(nx.connected_components(g_from_cc), key=len)\n",
    "largest_connected_component_subgr_cc = g_from_cc.subgraph(largest_connected_component_nodes_cc).copy()\n",
    "show_graph(largest_connected_component_subgr_cc, layout_fun_ = nx.spring_layout)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d10305",
   "metadata": {},
   "source": [
    "## Transfer Entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e685403",
   "metadata": {},
   "source": [
    "Transfer entropy could be described as how much the outcome of time series Y is described by time series X, considering L timepoints in history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6915ca90",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Comparing a spike train to itself returns {transfer_entropy(clusters_spikes_matrix[0], clusters_spikes_matrix[0])}\")\n",
    "print(f\"Comparing a spike train to a distant spike train returns {transfer_entropy(clusters_spikes_matrix[0], clusters_spikes_matrix[-1])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a45e32f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_limit = 10\n",
    "sample_count = 0\n",
    "for i, _ in enumerate(clusters_spikes_matrix):\n",
    "    for j in range(i+1, min(i+100, len(clusters_spikes_matrix))):\n",
    "        if transfer_entropy(clusters_spikes_matrix[i], clusters_spikes_matrix[j]) > 0: #TODO\n",
    "            sample_count += 1\n",
    "            print(f\"Spike train {i} is functionally connected to spike train {j}\")\n",
    "            if sample_count >= sample_limit:\n",
    "                break\n",
    "    if sample_count >= sample_limit:\n",
    "        break \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01991965",
   "metadata": {},
   "source": [
    "We can use transfer entropy to construct an adjacency matrix and a graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56bd36f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "adjacency_matrix_from_te = adjacency_matrix_from_pairwise(clusters_spikes_matrix, transfer_entropy)\n",
    "g_from_te = nx.from_numpy_array(adjacency_matrix_from_te, create_using=nx.DiGraph)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68468834",
   "metadata": {},
   "source": [
    "Let's visualize the resulting graph and largest connected component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab1cb9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_graph(g_from_te, (5, 3))\n",
    "\n",
    "largest_connected_component_nodes_te = max(nx.weakly_connected_components(g_from_te), key=len)\n",
    "largest_connected_component_subgr_te = g_from_cc.subgraph(largest_connected_component_nodes_te).copy()\n",
    "show_graph(largest_connected_component_subgr_te, layout_fun_ = nx.spring_layout)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a85467f8",
   "metadata": {},
   "source": [
    "## Graph Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa48f0b",
   "metadata": {},
   "source": [
    "The cross correlation and transfer entropy algorithms yielded the `largest_connected_component_subgr_cc` and `largest_connected_component_subgr_te` respectively, where each is the maximal connected subgraph of the identified partial connectome. Each graph includes nodes corresponding to putative neurons, and edges corresponding to putative neural connections. Moving forward, we'll focus on the transfer-entropy-produced graph, `G`, analyzing it using basic network science techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1c9982",
   "metadata": {},
   "outputs": [],
   "source": [
    "G = largest_connected_component_subgr_te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa13d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in G.nodes:\n",
    "    G.nodes[n][\"ind\"] = n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b930e71c",
   "metadata": {},
   "source": [
    "### Degree Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e6c4e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in G.nodes:\n",
    "    G.nodes[n]['degree'] = G.degree[n]\n",
    "\n",
    "print(f\"The degree distribution has max: {max(dict(G.degree()).values())}, min: {min(dict(G.degree()).values())}, and mean: {sum(dict(G.degree()).values())/len(dict(G.degree()).values())}\")\n",
    "\n",
    "def show_degree_distribution(G: nx.Graph, fp: Optional[str] = None) -> None:\n",
    "    \"\"\" \n",
    "    Adapted from CS 575 work, which was adapted from Hands-On Graph Neural Networks Using Python by Maxime Labonne, chapter 6.\n",
    "    \"\"\"\n",
    "    degree_list: list[int] = [y for (_,y) in G.degree] # type: ignore\n",
    "    \n",
    "    _, ax = plt.subplots()\n",
    "    ax.set_title('Degree Distribution')\n",
    "    ax.set_xlabel('Node degree')\n",
    "    ax.set_ylabel('Number of nodes')\n",
    "    \n",
    "    plt.hist(degree_list)\n",
    "\n",
    "    if fp:\n",
    "        plt.savefig(fp)\n",
    "        plt.close()\n",
    "    else:\n",
    "        plt.show()\n",
    "\n",
    "show_degree_distribution(G)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "897d2589",
   "metadata": {},
   "source": [
    "The network can be visualized by degree:\n",
    "\n",
    "![degree image](/Users/wesley/GitHub/BYU/ms-proj/replication/gephi/degree.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "164847cb",
   "metadata": {},
   "source": [
    "### Scale Free Property"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "becd1d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_degree_density(G: nx.Graph, log_log: bool = True) -> None:\n",
    "    degree_list: list[int] = [y for (_,y) in G.degree] # type: ignore\n",
    "    degree_dict: dict[int, int] = {}\n",
    "    for deg in degree_list:\n",
    "        if deg not in degree_dict.keys():\n",
    "            degree_dict[deg] = 0\n",
    "        degree_dict[deg] += 1\n",
    "\n",
    "    _, ax = plt.subplots()\n",
    "    ax.set_title(f'{\"Log-Log \" if log_log else \"\"}Degree Distribution')\n",
    "    ax.set_xlabel('Node degree')\n",
    "    ax.set_ylabel('Probability')\n",
    "\n",
    "    x = [degree for degree, _ in sorted(degree_dict.items())]\n",
    "    y = [count for _, count in sorted(degree_dict.items())]\n",
    "    if 0 not in x:\n",
    "        x.insert(0,0)\n",
    "        y.insert(0,0)\n",
    "    y = y / np.sum(y)\n",
    "    if log_log:\n",
    "        plt.loglog(x,y)\n",
    "    else:\n",
    "        plt.plot(x,y)\n",
    "\n",
    "show_degree_density(G, False)\n",
    "show_degree_density(G, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eaf91ef",
   "metadata": {},
   "source": [
    "This is not a scale-free network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4eea192",
   "metadata": {},
   "source": [
    "### Centrality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ee7402",
   "metadata": {},
   "outputs": [],
   "source": [
    "g_pagerank = nx.pagerank(G)\n",
    "\n",
    "for n, val in g_pagerank.items():\n",
    "    G.nodes[n]['centrality'] = val\n",
    "\n",
    "print(f\"The centrality distribution has max: {max(g_pagerank.values())}, min: {min(g_pagerank.values())}, and mean: {sum(g_pagerank.values())/len(g_pagerank.values())}\")\n",
    "\n",
    "def show_centrality_distribution(G: nx.Graph, fp: Optional[str] = None) -> None:\n",
    "    \"\"\" \n",
    "    Adapted from CS 575 work, which was adapted from Hands-On Graph Neural Networks Using Python by Maxime Labonne, chapter 6.\n",
    "    \"\"\"\n",
    "    centrality_list: list[int] = [G.nodes[n]['centrality'] for n in G.nodes] # type: ignore\n",
    "    \n",
    "    _, ax = plt.subplots()\n",
    "    ax.set_title('Centrality Distribution')\n",
    "    ax.set_xlabel('Node centrality')\n",
    "    ax.set_ylabel('Number of nodes')\n",
    "    \n",
    "    plt.hist(centrality_list)\n",
    "\n",
    "    if fp:\n",
    "        plt.savefig(fp)\n",
    "        plt.close()\n",
    "    else:\n",
    "        plt.show()\n",
    "\n",
    "show_centrality_distribution(G)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d05b6f",
   "metadata": {},
   "source": [
    "The network can be visualized by centrality:\n",
    "\n",
    "![centrality image](/Users/wesley/GitHub/BYU/ms-proj/replication/gephi/centrality.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2bdddb3",
   "metadata": {},
   "source": [
    "### Clustering Coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fadcc12",
   "metadata": {},
   "outputs": [],
   "source": [
    "g_clustering = dict(nx.clustering(G)) # type: ignore\n",
    "\n",
    "for n, val in g_clustering.items():\n",
    "    G.nodes[n]['clustering'] = val\n",
    "\n",
    "print(f\"The clustering distribution has max: {max(g_clustering.values())}, min: {min(g_clustering.values())}, and mean: {sum(g_clustering.values())/len(g_clustering.values())}\") # type: ignore\n",
    "\n",
    "def show_clustering_distribution(G: nx.Graph, fp: Optional[str] = None) -> None:\n",
    "    \"\"\" \n",
    "    Adapted from CS 575 work, which was adapted from Hands-On Graph Neural Networks Using Python by Maxime Labonne, chapter 6.\n",
    "    \"\"\"\n",
    "    clustering_list: list[int] = [G.nodes[n]['clustering'] for n in G.nodes] # type: ignore\n",
    "    \n",
    "    _, ax = plt.subplots()\n",
    "    ax.set_title('Clustering Distribution')\n",
    "    ax.set_xlabel('Node clustering coefficient')\n",
    "    ax.set_ylabel('Number of nodes')\n",
    "    \n",
    "    plt.hist(clustering_list)\n",
    "\n",
    "    if fp:\n",
    "        plt.savefig(fp)\n",
    "        plt.close()\n",
    "    else:\n",
    "        plt.show()\n",
    "\n",
    "show_clustering_distribution(G)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d6d401",
   "metadata": {},
   "source": [
    "### Eccentricity and the Small World Property"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0601627c",
   "metadata": {},
   "outputs": [],
   "source": [
    "g_eccentricity = dict(nx.eccentricity(G)) # type: ignore\n",
    "\n",
    "for n, val in g_eccentricity.items():\n",
    "    G.nodes[n]['eccentricity'] = val\n",
    "\n",
    "print(f\"The eccentricity distribution has max (Diameter): {max(g_eccentricity.values())}, min (Radius): {min(g_eccentricity.values())}, and mean: {sum(g_eccentricity.values())/len(g_eccentricity.values())}\") # type: ignore\n",
    "\n",
    "def show_eccentricity_distribution(G: nx.Graph, fp: Optional[str] = None) -> None:\n",
    "    \"\"\" \n",
    "    Adapted from CS 575 work, which was adapted from Hands-On Graph Neural Networks Using Python by Maxime Labonne, chapter 6.\n",
    "    \"\"\"\n",
    "    eccentricity_list: list[int] = [G.nodes[n]['eccentricity'] for n in G.nodes] # type: ignore\n",
    "    \n",
    "    _, ax = plt.subplots()\n",
    "    ax.set_title('Eccentricity Distribution')\n",
    "    ax.set_xlabel('Node eccentricity coefficient')\n",
    "    ax.set_ylabel('Number of nodes')\n",
    "    \n",
    "    plt.hist(eccentricity_list)\n",
    "\n",
    "    if fp:\n",
    "        plt.savefig(fp)\n",
    "        plt.close()\n",
    "    else:\n",
    "        plt.show()\n",
    "\n",
    "show_eccentricity_distribution(G)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a55496e4",
   "metadata": {},
   "source": [
    "### Partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57e5ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "communities = nx.algorithms.community.louvain_communities(G)\n",
    "for n in G.nodes:\n",
    "    for i, c in enumerate(communities): # type: ignore\n",
    "        if n in c:\n",
    "            G.nodes[n][\"community\"] = i\n",
    "    if G.nodes[n].get(\"community\") is None:\n",
    "        G.nodes[n][\"community\"] = \"9999\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e0ee71c",
   "metadata": {},
   "source": [
    "The network can be visualized by community:\n",
    "\n",
    "![community image](/Users/wesley/GitHub/BYU/ms-proj/replication/gephi/communities.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c69a93af",
   "metadata": {},
   "source": [
    "### Core-Periphery Structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd0189e",
   "metadata": {},
   "outputs": [],
   "source": [
    "k_core_info = dict(nx.core_number(G)) # type: ignore\n",
    "\n",
    "for n, val in k_core_info.items():\n",
    "    G.nodes[n]['kcore'] = val\n",
    "\n",
    "print(f\"The K core distribution has max: {max(k_core_info.values())}, min: {min(k_core_info.values())}, and mean: {sum(k_core_info.values())/len(k_core_info.values())}\") # type: ignore\n",
    "\n",
    "def show_kcore_distribution(G: nx.Graph) -> None: # type: ignore\n",
    "    \"\"\" \n",
    "    Adapted from CS 575 work, which was adapted from Hands-On Graph Neural Networks Using Python by Maxime Labonne, chapter 6.\n",
    "    \"\"\"\n",
    "    g_functional = nx.DiGraph() # type: ignore\n",
    "    g_functional.add_nodes_from([n for n in G.nodes])\n",
    "    g_functional.add_edges_from([(e[0], e[1]) for e in G.edges if (e[0], e[1]) not in g_functional.edges])\n",
    "    k_core_info = nx.core_number(g_functional)\n",
    "\n",
    "    k_core_scores = [v for _, v in k_core_info.items()]\n",
    "\n",
    "    _, ax = plt.subplots()\n",
    "    ax.set_title(\"K Core Structure\")\n",
    "    ax.set_xlabel('Max Node K Core')\n",
    "    ax.set_ylabel('Number of nodes')\n",
    "    plt.hist(k_core_scores)\n",
    "    plt.show()\n",
    "\n",
    "show_kcore_distribution(G)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a44f881",
   "metadata": {},
   "source": [
    "The network can be visualized by K core:\n",
    "\n",
    "![kcore image](/Users/wesley/GitHub/BYU/ms-proj/replication/gephi/kcores.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "944b50c5",
   "metadata": {},
   "source": [
    "Consistent with prior work in CS 575, this network shows a strong core-periphery structure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe2105a1",
   "metadata": {},
   "source": [
    "## Export to Gephi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaba91c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_fp = f\"/Users/wesley/GitHub/BYU/ms-proj/tmp/{str(uuid.uuid4())}.gexf\"\n",
    "assert os.path.exists(os.path.dirname(tmp_fp))\n",
    "nx.write_gexf(G, tmp_fp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
